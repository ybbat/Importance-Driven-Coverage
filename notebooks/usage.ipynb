{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "\n",
    "from importance_driven_coverage import attributors, clusterers, coverage\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2, 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2, 2)\n",
    "\n",
    "        x = x.view(-1, 16*5*5)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet5().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToPureTensor(),\n",
    "    T.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST(\"data\", download=True, transform=transforms, train=True)\n",
    "train, validation = data.random_split(train_dataset, [0.9, 0.1])\n",
    "train_loader = data.DataLoader(train, batch_size=128, num_workers=4)\n",
    "val_loader = data.DataLoader(validation, batch_size=128, num_workers=4)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\"data\", download=True, transform=transforms, train=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.305979 [  128/54000]\n",
      "loss: 0.958432 [ 5504/54000]\n",
      "loss: 0.255655 [10880/54000]\n",
      "loss: 0.279248 [16256/54000]\n",
      "loss: 0.206301 [21632/54000]\n",
      "loss: 0.048889 [27008/54000]\n",
      "loss: 0.115305 [32384/54000]\n",
      "loss: 0.133916 [37760/54000]\n",
      "loss: 0.048265 [43136/54000]\n",
      "loss: 0.077177 [48512/54000]\n",
      "loss: 0.022363 [53888/54000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.080670 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.022842 [  128/54000]\n",
      "loss: 0.129321 [ 5504/54000]\n",
      "loss: 0.118705 [10880/54000]\n",
      "loss: 0.065015 [16256/54000]\n",
      "loss: 0.146484 [21632/54000]\n",
      "loss: 0.020892 [27008/54000]\n",
      "loss: 0.023454 [32384/54000]\n",
      "loss: 0.142362 [37760/54000]\n",
      "loss: 0.008944 [43136/54000]\n",
      "loss: 0.090765 [48512/54000]\n",
      "loss: 0.012042 [53888/54000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.075657 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.021661 [  128/54000]\n",
      "loss: 0.034378 [ 5504/54000]\n",
      "loss: 0.075865 [10880/54000]\n",
      "loss: 0.043923 [16256/54000]\n",
      "loss: 0.128253 [21632/54000]\n",
      "loss: 0.025787 [27008/54000]\n",
      "loss: 0.006838 [32384/54000]\n",
      "loss: 0.126970 [37760/54000]\n",
      "loss: 0.009413 [43136/54000]\n",
      "loss: 0.061560 [48512/54000]\n",
      "loss: 0.002071 [53888/54000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.067148 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.95)\n",
    "\n",
    "def train(dataloader, model, criterion, opt):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            if batch % (num_batches//10) == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def val(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += criterion(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, criterion, optimizer)\n",
    "    val(val_loader, model, criterion)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ybbat/.micromamba/envs/idc/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "layer = model.fc3\n",
    "n = 8\n",
    "\n",
    "attributor = attributors.CaptumLRPAttributor(attribute_kwargs={\"attribute_to_layer_input\": True})\n",
    "clusterer = clusterers.KMeansClustererSimpleSilhouette()\n",
    "\n",
    "idc = coverage.ImportanceDrivenCoverage(model, attributor, clusterer)\n",
    "\n",
    "score, cov = idc.calculate(train_loader, test_loader, layer, n, layer_input=True,\n",
    "                           attributions_path=\"attributions.pt\",\n",
    "                           centroids_path=\"centroids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trans = transforms = T.Compose([\n",
    "    T.RandAugment()\n",
    "])\n",
    "score_after_trans, cov_after_trans = idc.calculate(train_loader, test_loader, layer, n, transform=new_trans, layer_input=True,\n",
    "                                                   attributions_path=\"attributions.pt\",\n",
    "                                                   centroids_path=\"centroids.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65536\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(idc.total_combs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score without transform: 0.02386474609375\n",
      "Score with transform: 0.02386474609375\n",
      "Combinations gained from transform: 484\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score without transform: {score}\")\n",
    "print(f\"Score with transform: {score}\")\n",
    "print(f\"Combinations gained from transform: {len(cov_after_trans - cov)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
